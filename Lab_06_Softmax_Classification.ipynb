{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-06 Softmax Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZCtKHQw9CiPohQl3y3hf+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxTVXvoweyDD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IhNYHtaji4K",
        "outputId": "07a18d26-f3f7-4efb-e978-f29cf86f5ea6"
      },
      "source": [
        "# For reproducibility\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f2534205af0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFd95dj7GBNy"
      },
      "source": [
        "## Discrete Probability Distribution\n",
        "- point 자체가 확률이 됨. (연속확률분포에서는 point 자체가 아니라 어떤 범위가 확률이 되는 것)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a5BXzecGmQA"
      },
      "source": [
        "## Softmax\n",
        "\n",
        "P(주먹|가위), P(가위|가위), P(보|가위)를 알 수 있다면 상대가 가위를 냈을 때 그 다음에 무엇을 낼지 예측해볼 수 있음. 이러한 정보를 바탕으로 해당 확률 분포의 pdf를 근사해볼 수 있음.\n",
        "\n",
        "- 일반적인 argmax의 방식으로는 max인 element에는 1, 그렇지 않은 경우에는 0이 되지만 softmax는 각각의 element들의 값이 합쳐서 1이 되게끔 구해질 것임."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V36mnuoFHK61",
        "outputId": "b0d7bd41-c6e5-4c8b-db08-49576324a2f2"
      },
      "source": [
        "z = torch.FloatTensor([1, 2, 3])\n",
        "\n",
        "hypothesis = F.softmax(z, dim = 0)\n",
        "print(hypothesis) # 확률 값을 갖게 됨."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0900, 0.2447, 0.6652])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDXwrnyzIV8k"
      },
      "source": [
        "## Cross Entropy\n",
        "- Loss function으로 사용함.\n",
        "- 두 개의 확률분포가 주어졌을 때, 두 확률 분포가 얼마나 비슷한지 나타내주는 값.\n",
        "P에서 sampling한 x 값을 Q에 대입하여 log를 취한 후 합한 값을 나타냄.\n",
        "- Cross Entropy를 minimize하면 원하는 확률 분포로 근사할 수 있음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__96rXyvIQp_",
        "outputId": "c584b70d-7b5a-4baf-b441-265091fd9894"
      },
      "source": [
        "z = torch.rand(3, 5, requires_grad = True) # 5 classes, 3 samples\n",
        "hypothesis = F.softmax(z, dim=1)\n",
        "print(hypothesis) # prediction : y_hat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
            "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
            "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGKOSZTsIh_h",
        "outputId": "bfc5acca-0103-4f0e-d859-40852009dbea"
      },
      "source": [
        "# generate answer randomly\n",
        "# 각각의 sample에 대해 정답 : index 0, 2, 1이 정답이 될 것임.\n",
        "y = torch.randint(5, (3, )).long()\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RwqV6cgJXAC",
        "outputId": "47d53bc0-dd53-48e8-b0c4-05df9934a5ac"
      },
      "source": [
        "# one hot vector로 만들어주기\n",
        "y_one_hot = torch.zeros_like(hypothesis)\n",
        "y_one_hot.scatter_(1, y.unsqueeze(1), 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa0d2rWoJyeg",
        "outputId": "1c492c9f-be14-4926-8b1a-fa7bce12035a"
      },
      "source": [
        "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
        "print(cost)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYKyBDQcKfLm"
      },
      "source": [
        "위 과정을 torch에서 제공해주는 `log_softmax()`로 쉽게 수행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjfI7-GjKe4D",
        "outputId": "24b4ff61-8824-4cc5-cb69-2a6fbc6c76e9"
      },
      "source": [
        "F.log_softmax(z, dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
              "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
              "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
              "       grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbU8xJb1KwOv"
      },
      "source": [
        "# 위의 cost function을 다음과 같이 바꿀 수 있다.\n",
        "cost = (y_one_hot * -F.log_softmax(z, dim=1)).sum(dim=1).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0v0knWJLAqp",
        "outputId": "dd45daa9-981b-4907-ef6e-d706948d697e"
      },
      "source": [
        "# 위보다 더 쉽게 작성하려면 nll_loss 사용하여 sum과 mean을 생략할 수 있음.\n",
        "# nll : negative log likelihood loss\n",
        "F.nll_loss(F.log_softmax(z, dim=1), y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.4689, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRWuqjeZLVoH",
        "outputId": "c8ddc69a-2e7e-4357-b495-cd251ae5990c"
      },
      "source": [
        "# F.cross_entropy를 사용하면 F.log_softmax()와 F.nll_loss()를 한 번에 작동할 수 있음.\n",
        "F.cross_entropy(z, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.4689, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzB2pzCHLpwi"
      },
      "source": [
        "`cross_entropy()`가 간편하긴 하지만, neural network에서는 softmax의 값 이전 값을 사용해서 prediction 확률 값을 구해야할 수도 있어서, 필요에 따라서 알맞은 것을 사용해야함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O15ZZRI4MICP"
      },
      "source": [
        "## Training with Low-level Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_UbNl5KMG43"
      },
      "source": [
        "x_train = [[1, 2, 1, 1],\n",
        "           [2, 1, 3, 2],\n",
        "           [3, 1, 3, 4],\n",
        "           [4, 1, 5, 5],\n",
        "           [1, 7, 5, 5],\n",
        "           [1, 2, 5, 6],\n",
        "           [1, 6, 6, 6],\n",
        "           [1, 7, 7, 7]] # dim = 4\n",
        "y_train = [2, 2, 2, 1, 1, 1, 0, 0] # one hot vector에서 1이 있는 위치\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.LongTensor(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkWdHagJNJDE"
      },
      "source": [
        "## High level implementation with nn.Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2EOfSEPLVqQ",
        "outputId": "da33119e-ef12-43db-f995-1d73ed2b4c68"
      },
      "source": [
        "class SoftmaxClassifierModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(4, 3) # Output이 3!\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)\n",
        "    \n",
        "model = SoftmaxClassifierModel()\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "  # H(x) 계산\n",
        "  prediction = model(x_train)\n",
        "\n",
        "  # cost 계산\n",
        "  cost = F.cross_entropy(prediction, y_train)\n",
        "\n",
        "  # cost로 H(x)로 개선\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # 20번마다 로그 출력\n",
        "  if epoch % 100 == 0:\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, cost.item()\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 Cost: 1.849513\n",
            "Epoch  100/1000 Cost: 0.689894\n",
            "Epoch  200/1000 Cost: 0.609259\n",
            "Epoch  300/1000 Cost: 0.551218\n",
            "Epoch  400/1000 Cost: 0.500141\n",
            "Epoch  500/1000 Cost: 0.451947\n",
            "Epoch  600/1000 Cost: 0.405051\n",
            "Epoch  700/1000 Cost: 0.358733\n",
            "Epoch  800/1000 Cost: 0.312912\n",
            "Epoch  900/1000 Cost: 0.269522\n",
            "Epoch 1000/1000 Cost: 0.241922\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}